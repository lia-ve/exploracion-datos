---
title: "Análisis exploratorio de datos"
subtitle: "Extrayendo significado del dato bruto"
lang: es
author:
  - name: Rainer Palm
    email: rdppetrizzo@gmail.org
    affiliations: Laboratorio Venezolano de Inteligencia Artificial
format:
  revealjs: 
    theme: [default, custom.scss]
    slide-number: true
    chalkboard: 
      buttons: false
    preview-links: auto
    logo: data/images/logo_lia.png
    footer: '[Laboratorio Venezolano de Inteligencia Artificial](https://www.lia-ve.org)'
---

## Introducción

En esta sesión, desarrollaremos un ejemplo de como se realiza un analísis exploratorio de datos (AED) con Python, trabajando con datos reales del Índice de Calidad del Aire de Ciudad de México. 

Verás como se realiza cada paso de este proceso, desde su **carga** y **preparación**, hasta su **visualización**, culminando con un **análisis** de correlaciones mediante un modelo de regresión lineal.

## Objetivos

1. Hacer visualizaciones y análisis de los datos para ver relaciones, distribuciones y tendencias

2. Detectar posibles patrones y correlaciones

3. Interpretar hallazgos y generar hipótesis

## Análisis exploratorio de datos

Es un proceso crítico en el cual se **exploran e investigan** los datos sin suposiciones previas para llegar a una comprensión más completa de estos (y sus patrones, relaciones, características, etc.). 

Así se puede decidir de manera informada:

- Que variables son **relevantes**?

- Como debería **tratar** las variables?

- Que **técnicas estadísticas o modelos** se deberían utilizar con estos datos?

## Descripciones generales del conjunto de datos {.scrollable}

Usaremos el conjunto de datos después de que este ha sido sometido a un proceso de revisión y limpieza.

```{python}
#| echo: true
import pandas as pd
import numpy as np
df_imeca = pd.read_parquet('data/imeca1996_2022.parquet')
```

---

Primero, deberíamos hacernos una idea general de que existe dentro del conjunto. Por ejemplo, viendo cuales son sus **dimensiones**, cuales son las **características** o variables que existen en el, y/o cuales son los tipos de datos de estás variables.

```{python}
#| echo: true
# El metodo .shape nos mostrará cuales son las dimensiones del conjunto.
# El primer numero da la cantidad de filas, y el segundo número da la cantidad de columnas.
df_imeca.shape
```

---

```{python}
#| echo: true
# Con .head, podemos hacer una vista preliminar al conjunto.
# En este caso, nos mostrará las primeras 5 filas.
df_imeca.head(5)
```

---

Para estar seguros de la cantidad de campos, los nombres de los campos, y los tipo de datos de los campos, podemos usar el metodo `.info`:

```{python}
#| echo: true
df_imeca.info()
```

---

¿Cual es el porcentaje de valores nulos dentro del dataset? Podemos realizar una operación sencilla con numpy para averiguarlo:

```{python}
#| echo: true
np.round(df_imeca.isnull().mean() * 100,1)
```

---

Después, sería buena idea evaluar de manera general el contenido de los datos.

Panda nos ofrece un método llamado `.describe` que automaticamente calcula e muestra medidas básicas de los campos del conjunto.

Esto es una forma útil de ver de manera inmediata los valores minimos y máximos de cada campo, además de su valor medio, la desviación estándar y la cantidad de filas que contiene.

---

```{python}
#| echo: true
df_imeca.select_dtypes(include='number').describe()
```

---

Las anteriores medidas no podrán ser generadas para variables categóricas. 

Sí queremos saber que categorias se presentan con más frecuencia, podemos usar el metodo `.value_counts`:

```{python}
#| echo: true
# Lo usaremos junto al metodo .apply para poder ver la cantidad de valores a lo largo de varias columnas.
df_imeca.select_dtypes(include='object').apply(pd.Series.value_counts)
```

---

Utilizando el argumento `normalize=True`, podemos ver estas categorias en función del porcentaje de filas que ocupan:

```{python}
#| echo: true
df_imeca.select_dtypes(include='object').apply(pd.Series.value_counts, normalize=True).mul(100).round(1).astype(str) + '%'
```

---

Es útil observar la distribución de los datos mediante un **histograma**, o mediante un **gráfico de densidad**:

```python
sureste_columnas = [i for i in df_imeca.select_dtypes(include='number').columns if "Sureste" in i]

hist= df_imeca[sureste_columnas].hist(bins=30, figsize=(16,8))
```

```python
kde = df_imeca[sureste_columnas].plot(kind='kde',subplots=True,layout=(3,2),figsize=(16,8))
```

Esto resulta util para identificar valores atípicos, saber que pruebas estadísticas usar, ver posibles errores con el proceso de ingesta y recolecta de datos, y además para poder escoger de manera informada el modelo predictivo a utilizar.

---

```{python}
sureste_columnas = [i for i in df_imeca.select_dtypes(include='number').columns if "Sureste" in i]

hist= df_imeca[sureste_columnas].hist(bins=30, figsize=(16,8))
```

---

```{python}
kde = df_imeca[sureste_columnas].plot(kind='kde',subplots=True,layout=(3,2),figsize=(16,8))
```

## Operaciones básicas con datasets y Pandas

Post-revisión, necesitaremos usar algunos metodos básicos de Pandas para poder explorar los datos con mayor detalle.

Principalmente, esto involucra el uso de **máscaras**:

```{python}
#| echo: true
# Seleccionar columnas específicas (por ejemplo, variables de la zona Sureste)
subset = df_imeca[[col for col in df_imeca.columns if "Sureste" in col]]
```

## {.smaller}

```{python}
subset
```

---

Podemos también usar varias condiciones, usando operadores de bit (& siendo igual a AND, y | siendo igual a OR).

```{python} 
#| echo: true
import datetime

# Filtrar los datos por un rango de tiempo específico
filtered_data = df_imeca[(df_imeca['Fecha-Hora'] > np.datetime64("1998-01-01")) & (df_imeca['Fecha-Hora'] < np.datetime64("2002-01-01"))]
filtered_data.set_index('Fecha-Hora',inplace=True)
```

## {.smaller}

```{python} 
filtered_data
```

---

Otra operación básica de gran importancia es la **agrupación**. Esta divide el dataframe en segmentos (grupos) y luego les aplica una función para convertir estos segmentos en un solo valor.

```{python} 
#| echo: true
# Agrupar el dataframe según los meses en las columnas de fecha y hora. 
# La función que agrupa las filas en este caso es "mean", lo cual significa
# que para cada més, tendremos la media de todos los valores de este més.
df_imeca_mes = df_imeca.select_dtypes(include='number').groupby(df_imeca['Fecha-Hora'].dt.to_period('M')).mean()
df_imeca_mes.head(3)
```

---

Finalmente, la ultima operación básica que veremos es el **ordenamiento**. Simplemente se trata de ordenar el dataframe en order ascendente o descendente según una (o varias) columnas.

```{python}
#| echo: true
# Ordenar los datos por el nivel de Ozono en el Noroeste de mayor a menor
df_imeca.sort_values(by='Noroeste Ozono', ascending=False).head(3)
```


## Visualizaciones temporales

¿A lo largo de los años, la contaminación tiene una tendencia creciente o decreciente?
Podemos emplear una visualización sencilla para responder esta incognita.

```{python}
#| echo: true
# Realizamos una agrupación según més para ver tendencias según ese periodo de tiempo. 
df_imeca_mes = (
    df_imeca.select_dtypes(include=["datetime", float])
    .resample("ME", on="Fecha-Hora")
    .mean()
)
df_imeca_mes['Noroeste PM10'].reset_index().set_index('Fecha-Hora').plot(figsize=(14,2))
```

---

En casos como este, donde resulta dificil observar su comportamiento a lo largo del tiempo a simple vista, podemos generar una linea de tendencia a partir de una regresión lineal simple.

```{python}
#| echo: true
import numpy as np
from sklearn.linear_model import LinearRegression
from matplotlib import pyplot as plt

df_imeca_mes = df_imeca_mes.dropna(subset=["Noroeste PM10"])

# Se usa el metodo 'reshape' para asegurar que sklean trate las variables como unidimensionales
X = np.arange(len(df_imeca_mes)).reshape(-1, 1)  # Tiempo en forma de índice
y = df_imeca_mes["Noroeste PM10"].values.reshape(-1, 1)  # Ozono en la región Noroeste

modelo = LinearRegression()
modelo.fit(X, y)

predicciones = modelo.predict(X)
```

---

```{python}
plt.figure(figsize=(14, 5))
plt.plot(
    df_imeca_mes.index,
    df_imeca_mes["Noroeste PM10"],
    label="Noroeste PM10",
    color="blue",
)
plt.plot(
    df_imeca_mes.index,
    predicciones,
    label="Tendencia Lineal",
    color="red",
    linestyle="--",
)
plt.title("Tendencia del Ozono con Ajuste Lineal en la Región Noroeste", fontsize=16)
plt.xlabel("Fecha")
plt.ylabel("Concentración de Ozono")
plt.grid(True)
plt.legend()
plt.show()

print(f"Pendiente de la tendencia: {modelo.coef_[0][0]:.4f}")
```

Por el valor de la pendiente que fue encontrado, se comprueba que la tendencia para este contaminante en la región Noroeste es creciente.

---

¿Que años presentarón peor calidad del aire, en promedio? Aquí es prudente hacer un gráfico de barras:

```{python}
#| echo: true
import matplotlib as mpl
from matplotlib import pyplot as plt

#| echo: true
# Realizamos una agrupación según més para ver tendencias según ese periodo de tiempo. 
df_imeca_year = (
    df_imeca.select_dtypes(include=["datetime", float])
    .resample("Y", on="Fecha-Hora")
    .mean()
)
noroeste_pm10_year = df_imeca_year['Noroeste PM10'].reset_index().set_index('Fecha-Hora')
# Filtramos para un periodo de tiempo más pequeño (para ver con más claridad la gráfica resultante)
noroeste_pm10_year = noroeste_pm10_year[(noroeste_pm10_year.index > '1999-12-01') & (noroeste_pm10_year.index < '2022-01-01')]
```

---

```{python}
#| echo: true
f, ax = plt.subplots()
# Definición de gráfica de barra
ax.bar(noroeste_pm10_year.index, noroeste_pm10_year.squeeze(), width=160, edgecolor='black')
# Definición de leyenda
ax.set_title('Impacto promedio del contaminante PM10 en el Noroeste por año')
ax.set_ylabel("ICA")
# Formateando años en el eje X correctamente
ax.xaxis.set_major_locator(mpl.dates.YearLocator())
ax.xaxis.set_major_formatter(mpl.dates.DateFormatter('%Y-%m'))
plt.xticks(rotation = 30)
f.set_size_inches(16,3)
plt.show()
```

## Comparación de categorías

En el preprocesamiento del dataset, se crearon categorías de calidad del aire a partir de las columnas numéricas. Estás son: *Buena, Mediocre, Mala, y Peligrosa*. Se toman a partir de la misma definición del Indice de Calidad del Aire (de parte del EPA USA).

Podemos usar estas categorías para comparar cuales regiones tienen más ocurrencias de una calidad de aire *peligrosa* a lo largo del año. 

---

```{python}
#| echo: true
regions = ["Noreste", "Noroeste", "Centro", "Sureste", "Suroeste"]

quality_count_dfs = []

for region in regions:
    calidad = [i for i in df_imeca if region in i and "Calidad" in i]
    value_counts = df_imeca[calidad][df_imeca['Fecha-Hora'].dt.year == 2020].apply(pd.Series.value_counts, dropna=True).replace(np.nan, 0)
    value_counts = value_counts.sum(axis=1).rename(region)
    quality_count_dfs.append(value_counts)

quality_count_df = pd.DataFrame(quality_count_dfs)
quality_count_df = quality_count_df.pivot_table(columns=regions)
quality_count_df
```

---

```{python}
#| echo: true
quality_count_df.plot(kind='bar',
        title='Comparación de la calidad del aire entre regiones (2020)', figsize=(16,5))
```

---

¿Que contaminantes presentan un mayor riesgo para la población? Podemos hacer una comparación similar, pero según contaminantes en lugar de según regiones:

```{python}
#| echo: true
regions = ["Ozono", "dióxido de azufre", "dióxido de nitrógeno", "monóxido de carbono", "PM10", "PM25"]

quality_count_dfs = []

for region in regions:
    calidad = [i for i in df_imeca if region in i and "Calidad" in i]
    value_counts = df_imeca[calidad][df_imeca['Fecha-Hora'].dt.year == 2020].apply(pd.Series.value_counts, dropna=True).replace(np.nan, 0)
    value_counts = value_counts.sum(axis=1).rename(region)
    quality_count_dfs.append(value_counts)

quality_count_df = pd.DataFrame(quality_count_dfs)

```

---

```{python}
#| echo: true
quality_count_df.pivot_table(columns=regions).plot(kind='bar',
        title='Comparación del impacto en la calidad del aire (2020)', figsize=(16,5))
```

## Análisis de correlaciones

¿Existe alguna correlación, negativa o positiva, entre la presencia de los contaminantes, o entre regiones? Sí usamos campos númericos, una aproximación sencilla para responder esta pregunta es la **matriz de correlación**. 

```python
# Calcular la matriz de correlación para las variables numéricas en el dataframe
import seaborn as sns

# Agarramos varias columnas y le aplicamos el metodo .corr(), que por defecto usa 
# Pearson para calcular la correlación.
correlacion = df_imeca[
    ["Noroeste Ozono", "Noroeste PM10", "Centro Ozono", "Centro PM10", "Sureste PM25"]
].corr()

plt.figure(figsize=(15, 5))

# Creación de heatmap mediante seaborn
sns.heatmap(correlacion, annot=True, cmap="coolwarm", linewidths=0.5)
plt.title(
    "Matriz de Correlación entre Contaminantes en Diferentes Regiones", fontsize=16
)
plt.show()
```

---

```{python}
# Calcular la matriz de correlación para las variables numéricas en el dataframe
import seaborn as sns

# Agarramos varias columnas y le aplicamos el metodo .corr(), que por defecto usa Pearson para calcular la correlación
correlacion = df_imeca[
    ["Noroeste Ozono", "Noroeste PM10", "Centro Ozono", "Centro PM10", "Sureste PM25"]
].corr()

plt.figure(figsize=(15, 7))

# Creación de heatmap mediante seaborn
sns.heatmap(correlacion, annot=True, cmap="coolwarm", linewidths=0.5)
plt.title(
    "Matriz de Correlación entre Contaminantes en Diferentes Regiones", fontsize=16
)
plt.show()
```

---

Otra forma de observar relaciones entre multiples variables númericas es un **diagrama de dispersión**.

Al gráficar una variable contra otra, podemos, según rasgos que presente la visualización resultante, ver si están correlacionadas positiva o negativamente.

Además, es útil para identificar otros patrones en los datos, como outliers, clusters, o huecos.

---

```python
df_imeca_feb2009 = df_imeca[(df_imeca['Fecha-Hora'] > '2009-01-31') & (df_imeca['Fecha-Hora'] < '2009-03-01')]


x = df_imeca_feb2009['Sureste Ozono']
y = df_imeca_feb2009['Sureste monóxido de carbono']
colors = np.random.rand(x.shape[0])

plt.figure(figsize=(15,5))
plt.scatter(x, y, c=colors)
plt.title('ICA del Ozono y del Monóxido de Carbono en la región Sureste')
plt.xlabel('Sureste Ozono')
plt.ylabel('Sureste Monóxido de Carbono')
plt.grid(True)
plt.legend()
plt.show()
```

---

```{python}
df_imeca_feb2009 = df_imeca[(df_imeca['Fecha-Hora'] > '2009-01-31') & (df_imeca['Fecha-Hora'] < '2009-03-01')]


x = df_imeca_feb2009['Sureste Ozono']
y = df_imeca_feb2009['Sureste monóxido de carbono']
colors = np.random.rand(x.shape[0])

plt.figure(figsize=(15,5))
plt.scatter(x, y, c=colors)
plt.title('ICA del Ozono y del Monóxido de Carbono en la región Sureste')
plt.xlabel('Sureste Ozono')
plt.ylabel('Sureste Monóxido de Carbono')
plt.grid(True)
plt.legend()
plt.show()
```

---

Alternativamente, si estamos lidiando con datos categóricos, podemos generar una **tabla de contingencia**, que nos mostrará cuantas veces dos campos coinciden en sus categórias:

```{python}
#| echo: true
tabla_contingencia = pd.crosstab(df_imeca["Sureste Ozono Calidad"], df_imeca["Sureste PM10 Calidad"])
print(tabla_contingencia)
```

