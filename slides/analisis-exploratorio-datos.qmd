---
title: "Análisis exploratorio de datos"
subtitle: "Extrayendo significado del dato bruto"
lang: es
author:
  - name: Rainer Palm
    email: rdppetrizzo@gmail.org
    affiliations: Laboratorio Venezolano de Inteligencia Artificial
format:
  revealjs: 
    theme: [default, custom.scss]
    slide-number: true
    chalkboard: 
      buttons: false
    preview-links: auto
    logo: data/images/logo_lia.png
    footer: '[Laboratorio Venezolano de Inteligencia Artificial](https://www.lia-ve.org)'
---

## Introducción

En esta sesión, desarrollaremos un ejemplo de como se realiza un analísis exploratorio de datos (AED) con Python, trabajando con datos reales del Índice de Calidad del Aire de Ciudad de México. 

Verás como se realiza cada paso de este proceso, desde su **carga** y **preparación**, hasta su **visualización**, culminando con un **análisis** de correlaciones mediante un modelo de regresión lineal.

## Objetivos

1. Cargar y hacer una revisión de los datos

2. Limpiar, imputar o normalizar datos

3. Hacer visualizaciones y análisis generales de los datos

4. Emplear gráficos más especificos para ver relaciones, distribuciones y tendencias

5. Detección de patrones y correlaciones

6. Interpretación y generación de hipótesis

## Análisis exploratorio de datos

Es un proceso crítico en el cual se **exploran e investigan** los datos sin suposiciones previas para llegar a una comprensión más completa de estos (y sus patrones, relaciones, características, etc.). 

Así se puede decidir de manera informada:

- Que variables son **relevantes**?

- Como debería **tratar** las variables?

- Que **técnicas estadísticas o modelos** se deberían utilizar con estos datos?

## Ejemplo de carga

``` python 
# Este conjunto de datos se puede descargar
# directamente desde Python.
from urllib.request import urlretrieve
from pathlib import Path

import numpy as np
import pandas as pd
from tqdm import tqdm

# Se descargan todos los archivos excel para los años 1996-2023.
for year in tqdm(range(1996, 2023)):
    url = f"http://www.aire.cdmx.gob.mx/descargas/basesimeca/imeca{year}.xls"
    filename = f"data/{Path(url).name}"
    urlretrieve(url, filename)

# Se crea una lista de dataframes a partir de los archivos descargados.
dfs = []
for year in tqdm(range(1996, 2023)):
    df = pd.read_excel(f"data/imeca{year}.xls")
    dfs.append(df)
```

## Vista preliminar del dataframe {.smaller}

```{python}
import numpy as np
import pandas as pd
from tqdm import tqdm

dfs = []
for year in tqdm(range(1996, 2023)):
    df = pd.read_excel(f"data/imeca{year}.xls")
    dfs.append(df)

df_imeca = pd.concat(dfs, ignore_index=True)
df_imeca
```

---

```{python}
df_imeca.info()
```

Hay varios problemas: 

- Cantidad relativamente grande de valores nulos para algunas columnas

- Los nombres de las columnas no han sido capitalizados uniformemente.

- Los valores nulos acá están marcados con $-99$.

## Combinación de columnas similares

- Capitalizaciones no-uniformes

```{python}
#| echo: true
df_imeca.Fecha = df_imeca.Fecha.combine_first(df_imeca.FECHA)

zonas = ["Noroeste", "Noreste", "Centro", "Suroeste", "Sureste"]
for zona in zonas:
    df_imeca[f"{zona} Ozono"] = df_imeca[f"{zona} Ozono"].combine_first(
        df_imeca[f"{zona} ozono"]
    )

df_imeca = df_imeca.drop(columns=["FECHA"] + [f"{zona} ozono" for zona in zonas])
```

- Unidades de fecha-hora

```{python}
#| echo: true
## Específicamos que la unidad es en horas al momento de realizar la conversión
df_imeca.Hora = pd.to_timedelta(df_imeca.Hora, unit='h')

df_imeca["Fecha-Hora"] = df_imeca.Fecha + df_imeca.Hora
## Esto sumará el valor de Fecha con el valor de Hora para cada índice.
df_imeca = df_imeca.drop(columns=["Fecha", "Hora"])
## Borramos columnas innecesarias de nuevo.
```

## Dataframe post-procesamiento {.smaller}

```{python}
df_imeca
```

## Detección de valores faltantes

- Marca de valores nulos:

```{python}
#| echo: true
df_imeca = df_imeca.replace(-99, np.nan)
```

- Numero de valores nulos por columna:

```{python}
df_imeca.isnull().sum()
```

## Transformaciones iniciales

Es posible que el conjunto de datos tenga variables que estén en diferentes escalas. Por ejemplo. que los contaminantes aquí tengan diferentes rangos de magnitud.

En estos casos, podemos aplicar una **normalización**, facilitada por la libreria scikit-learn.

Si se dejan los datos sin cambiar, podríamos ver tendencias erroneas en el análisis.

## Normalización

```{python}
#| echo: true
# MinMaxScaler realiza una transformación lineal para todo el conjunto de datos,
# por defecto resultando en que todos se transformen en valores desde el 0 al 1.
from sklearn.preprocessing import MinMaxScaler
contaminantes = [
    'Noroeste Ozono',
    'Noroeste dióxido de azufre',
    'Noroeste dióxido de nitrógeno',
    'Noroeste monóxido de carbono',
    'Noroeste PM10'
]
scaler = MinMaxScaler()

# Datos después de la normalización (van del 0 al 1)
scaler.fit_transform(df_imeca[contaminantes])
```

## Eliminación de outliers

Los valores atípicos pueden hacer que extraigamos suposiciones incorrectas de los datos. Para detectarlos, podemos utilizar el método del rango intercuartílico:

```python

lista_pm10 = [i for i in df_imeca.columns.tolist() if "PM10" in i]

# Método IQR para eliminar outliers
Q1 = df_imeca[lista_pm10].quantile(0.25)
Q3 = df_imeca[lista_pm10].quantile(0.75)
IQR = Q3 - Q1

# Definir los límites de detección de outliers
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Filtrar outliers
df_imeca[(df_imeca[lista_pm10] >= lower_bound) & (df_imeca[lista_pm10] <= upper_bound)]
```

## Operaciones básicas con datasets y Pandas

Post-revisión, necesitaremos usar algunos metodos básicos de Pandas para poder explorar los datos con mayor detalle.

Principalmente, esto involucra el uso de **máscaras**:

```{python}
#| echo: true
# Seleccionar columnas específicas (por ejemplo, variables de la zona Sureste)
subset = df_imeca[[col for col in df_imeca.columns if "Sureste" in col]]
subset.head(3)
```

---

```{python}
#| echo: true
import datetime

# Filtrar los datos por una fecha específica
filtered_data = df_imeca[df_imeca['Fecha-Hora'] < np.datetime64("1997-01-01")]
filtered_data.head(3)
```

## Agrupación de datos {.smaller}

Nos podemos auxiliar también de un proceso de agrupación o agregación mediante Pandas. Esto permitirá resumir estádisticas, lo cual suele ser útil para verlos según periodos de tiempo o categorías:

```{python}
#| echo: true
# Agrupar por mes y año
grouped_df = df_imeca.groupby(df_imeca['Fecha-Hora'].dt.to_period('M')).mean()
```

```{python}
#| echo: true
grouped_df.drop("Fecha-Hora",axis=1,inplace=True)
sureste_columns = [i for i in grouped_df.columns if "Sureste" in i]
grouped_df = grouped_df.reset_index().set_index("Fecha-Hora")
grouped_df[sureste_columns][grouped_df.index < "2000-01-01"].plot()
```

## Ordenamiento de datos

También podemos ordenar los datos en orden ascendiente o descendientes según uno o varios campos en Pandas, de la siguiente manera:

```{python}
# Ordenar los datos por el nivel de Ozono en el Noroeste de mayor a menor
df_imeca.sort_values(by='Noroeste Ozono', ascending=False)
```

## Guardado de resultados

Después del preprocesamiento, deberíamos guardar el dataframe (separado de los datos en bruto, por supuesto). 

Aunque se puede guardar en .csv o .xlsx, lo más común es guardarlo en formato .parquet, que acelerará bastante su carga en Pandas a futuro:

```python
df_imeca.to_parquet("data/imeca1996_2022.parquet")
```

Dependiendo del formato de los valores, es posible que no puedan ser contenidos en un parquet. Aquí resulta útil convertir a un archivo pickle:

```python
df_imeca.to_pickle("data/imeca1996_2022.pkl")
```

## Análisis descriptivo

Con el método `describe()`, podemos calcular estadísticas descriptivas básicas: esto incluye la media, la mediana, el minimo, el maximo, y la desviación estándar.

```{python}
#| echo: true
df_imeca.describe()
```

---

Podemos usar las máscaras anteriormente descritas si queremos ver estas estadísticas solo para determinadas columnas o periodos de tiempo.

```{python}
#| echo: true
df_imeca[sureste_columns][df_imeca['Fecha-Hora'] > "2010-01-01"].describe()
```

## Histogramas mediante Matplotlib

Un **histograma** representa de manera gráfica, con barras, la cantidad de veces que se repite un valor a lo largo del conjunto de datos.

```{python}
import matplotlib.pyplot as plt
#| echo: true

df_imeca[['Noroeste Ozono', 'Noroeste dióxido de azufre']].hist(bins=30, figsize=(12, 5))
plt.suptitle('Distribución de contaminantes en la región Noroeste', fontsize=16)
plt.show()
```

## Distribución de frecuencias

Sí queremos calcular la frecuencia de rangos de valores (o intervalos), podemos usar los metodos `value_counts` o `cut` antes de graficar..

```{python}
#| echo: true 
df_imeca['Noroeste Ozono'].value_counts(bins=10).plot()
```

## Clasificación de datos numéricos

Dependiendo del tipo de análisis o herramientas que se deseen utilizar para este, puede ser necesaria la creación de categorias dentro del conjunto de datos. 

```{python}
#| echo: true

# Aplicar una función que clasifique la calidad del aire en 'Buena', 'Mala', 'Peligrosa'
# Estas clasificaciones vienen de la Agencia de Protección Ambiental de Estados Unidos, y son distintas para cada campo.
def clasificar_calidad(pm10):
    if pm10 < 50:
        return 'Buena'
    elif 50 <= pm10 < 100:
        return 'Mala'
    else:
        return 'Peligrosa'

sureste_pm10_clasificado = df_imeca['Sureste PM10'].apply(clasificar_calidad)
sureste_pm10_clasificado
```

---

```{python}
#| echo: true
sureste_pm10_clasificado.value_counts().plot(kind='bar')

```

## Codificación de variables categóricas:

Para algunas tareas de predicción, es necesario transformar variables categóricas a númericas. Una tecnica común es la codificación one-hot, que convierte estás a una mascara de bits:

```{python}
#| echo: true
# Convertir las variables categóricas a variables dummy (one-hot encoding)
df_ohotest = pd.DataFrame.copy(df_imeca)
df_ohotest['Noreste PM10'] = df_ohotest['Noreste PM10'].apply(clasificar_calidad)
df_with_dummies = pd.get_dummies(df_ohotest, columns=['Noreste PM10'])
df_with_dummies[df_with_dummies.columns[-3:]]

```

## Generar matrices de correlación

Una matriz de correlación mide la dependencia lineal entre cada par de variables en un conjunto de datos. 

Es una herramienta visual importante para identificar relaciones lineales fuertes (positivas o negativas) entre variables.

Esta medida de correlación se lee de la siguiente manera:

- Si es cercano a 1, indica que cuando una de las variables sube, la otra también.

- Si es cercano a -1, indica que una variable baja cuando la otra sube. 

- Si es cercano a 0, indica que no hay correlación entre las variables.

---

```{python}
#| echo: true
# Calcular la matriz de correlación para las variables numéricas en el dataframe
correlacion = df_imeca[
    ["Noroeste Ozono", "Noroeste PM10", "Centro Ozono", "Centro PM10", "Sureste PM25"]
].corr()

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(14, 3))
sns.heatmap(correlacion, annot=True, cmap="coolwarm", linewidths=0.5)
plt.title(
    "Matriz de Correlación entre Contaminantes en Diferentes Regiones", fontsize=16
)
plt.show()
```

## Generar coeficientes de correlación

- El coeficiente de **Pearson** mide la relación lineal entre dos variables continuas. Asume que los datos son normalmente distribuidos.

```{python}
#| echo: true
pearson_corr = df_imeca["Noroeste Ozono"].corr(
    df_imeca["Noroeste PM10"], method="pearson"
)
print(f"Correlación de Pearson entre Noroeste Ozono y Noroeste PM10: {pearson_corr}")
```

Esta correlación se lee igual que en el ultimo ejemplo. Por lo tanto, aquí podemos ver que no hay correlación entre estás dos variables. 

---

```{python}
#| echo: true
pearson_corr = df_imeca["Noroeste PM10"].corr(
    df_imeca["Noroeste PM25"], method="pearson"
)
print(f"Correlación de Pearson entre Noroeste PM10 y Noroeste PM25: {pearson_corr}")
```

PM10 y PM25 son dos variables que miden esencialmente dos tamaños de lo mismo (particulas, de diametro de 1.0 µm y 2.5 µm, respectivamente), así que es esperable que tengan una correlación positiva. 

::: {.notes}
- El coeficiente de **Spearman** mide de manera no paramétrica la correlación, usando el rango de los valores en lugar de los valores directamente. Por lo tanto, no importa que su distribución.
:::

## Relación entre variables continuas y categóricas

Ya que estamos trabajando con un conjunto de variables continuas, primero debemos crear una variable categórica (niveles de ozono). Luego, graficaremos un boxplot para ver la distribución de otra variable continua según estas categorías.

```{python}
#| echo: true
df_imeca['Niveles_Ozono'] = pd.cut(
    df_imeca['Noroeste Ozono'], 
    bins=[0, 50, 100, 150, np.inf], 
    labels=['Bajo', 'Moderado', 'Alto', 'Muy Alto']
)

# Mostrar la distribución de la nueva variable categórica
print(df_imeca['Niveles_Ozono'].value_counts())
```

---

```{python}
#| echo: true
plt.figure(figsize=(14, 4))
sns.boxplot(x="Niveles_Ozono", y="Noroeste PM10", data=df_imeca)
plt.title(
    "Concentración de PM10 según los Niveles de Ozono en la Región Noroeste",
    fontsize=16,
)
plt.xlabel("Niveles de Ozono")
plt.ylabel("Concentración de PM10")
plt.show()
```

## Análisis de tablas de contingencia

Sí queremos observar la relación dos variables categóricas, podemos usar una tabla de contingencia, la cual nos dejará ver cuantas veces, según las filas, coinciden dos categorías determinadas.

```{python}
#| echo: true
# Crear categorías para los niveles de PM10 en la región Noroeste
df_imeca["Niveles_PM10"] = pd.cut(
    df_imeca["Noroeste PM10"],
    bins=[0, 50, 100, 150, 200],
    labels=["Bajo", "Moderado", "Alto", "Muy Alto"],
)

tabla_contingencia = pd.crosstab(df_imeca["Niveles_Ozono"], df_imeca["Niveles_PM10"])
print(tabla_contingencia)
```

## Análisis temporal

Sí quisieramos, por ejemplo, ver si disminuye o incrementa la contaminación en el aire a lo largo del tiempo, podríamos ordenar los datos según la columna de fecha-hora y/o agruparlos según més, semana, etc. 

## Agrupación según meses o años {.smaller}

Al agrupar los datos, se necesita sacar la media de los contaminantes para cada periodo de agrupación (meses o años) para posteriormente observar que tendencias existen.

```{python}
# Agrupar por mes y calcular la media de los contaminantes
df_imeca_mes = (
    df_imeca.select_dtypes(include=["datetime", float])
    .resample("ME", on="Fecha-Hora")
    .mean()
)

# Mostrar las primeras filas del DataFrame agrupado por mes
df_imeca_mes.head()
```

## Evolución temporal de los contaminantes

Sí utilizamos el dataframe generado anteriormente en una gráfica de linea, veriamos una representación de la evolución temporal de los contaminantes,

Por ejemplo, para el Ozono en la región Noroeste:

```{python}
import matplotlib.pyplot as plt

# Graficar la evolución del Ozono en la región Noroeste a lo largo del tiempo
plt.figure(figsize=(14, 5))
plt.plot(
    df_imeca_mes.index,
    df_imeca_mes["Noroeste Ozono"],
    label="Noroeste Ozono",
    color="blue",
)
plt.title(
    "Tendencia del Ozono en la Región Noroeste a lo largo del Tiempo", fontsize=16
)
plt.xlabel("Fecha")
plt.ylabel("Concentración de Ozono")
plt.grid(True)
plt.legend()
plt.show()
```

---

Podemos también generar una línea de tendencia, mediante una regresión lineal simple. De esa manera se verá de forma más clara si la tendencia es de disminución o aumento.

```{python}
import numpy as np
from sklearn.linear_model import LinearRegression

df_imeca_mes = df_imeca_mes.dropna(subset=["Noroeste Ozono"])

X = np.arange(len(df_imeca_mes)).reshape(-1, 1)  # Tiempo en forma de índice
y = df_imeca_mes["Noroeste Ozono"].values.reshape(-1, 1)  # Ozono en la región Noroeste

modelo = LinearRegression()
modelo.fit(X, y)

predicciones = modelo.predict(X)

plt.figure(figsize=(14, 5))
plt.plot(
    df_imeca_mes.index,
    df_imeca_mes["Noroeste Ozono"],
    label="Noroeste Ozono",
    color="blue",
)
plt.plot(
    df_imeca_mes.index,
    predicciones,
    label="Tendencia Lineal",
    color="red",
    linestyle="--",
)
plt.title("Tendencia del Ozono con Ajuste Lineal en la Región Noroeste", fontsize=16)
plt.xlabel("Fecha")
plt.ylabel("Concentración de Ozono")
plt.grid(True)
plt.legend()
plt.show()

print(f"Pendiente de la tendencia: {modelo.coef_[0][0]:.4f}")
```

## Comparación de periodos de tiempo

Si estás interesado en comparar periodos específicos (por ejemplo, años antes y después de una intervención), puedes agrupar los datos por años y calcular el promedio de los contaminantes en esos periodos:

```{python}
df_imeca_anual = (
    df_imeca.select_dtypes(include=["datetime", float])
    .resample("YE", on="Fecha-Hora")
    .mean()
)

plt.figure(figsize=(14, 5))
plt.bar(df_imeca_anual.index.year, df_imeca_anual["Noroeste Ozono"], color="skyblue")
plt.title("Concentración Anual Promedio de Ozono en la Región Noroeste", fontsize=16)
plt.xlabel("Año")
plt.ylabel("Concentración de Ozono")
plt.show()
```